{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Organization\n",
    "\n",
    "- Git(Lab): [git.fim.uni-passau.de](https://git.fim.uni-passau.de/)\n",
    "- Environment Management: [Conda (Anaconda)](https://conda.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows\n",
    "\n",
    "**Rules for storing data:**\n",
    "Depending on what kind of data you work with, how it is structured and how easily it can be obtained or prepared for usage, try to apply following rules for what you actually want to persist in git:\n",
    "- 1) if data is an online-resource and can be retrieved quickly and the data source can be considered reliable available for at least a year: store only a routine to retrieve data locally, e.g. *retrieve_data.py*\n",
    "- 2) if data is small, nicely readable and well-structured such as in a CSV or JSON file, store it directly in git so it is simply accessible when checking out your repository, e.g. *data/2020-05-05-some_structured_infos.json*. It always makes sense to note down where you retrieved data from, e.g. putting a file along with it *data/2020-05-05-some_structured_infos.md* which contains infos where you retrieved it from, what license might apply etc\n",
    "- 3) if data is large or binary, then store it only in git if you also use ``git lfs track`` (have a look for the documentation on [git lfs](https://git-lfs.github.com/))\n",
    "- 4) if your repository does not support git-lfs or you want to make data available, consider an online storage such as Amazon S3 or a data provider such as Zenodo\n",
    "\n",
    "**Workflows:**\n",
    "- a) **Data:** Retrieving (downloading) data locally\n",
    "- b) **Analysis:** load data in memory (partially) -> restructure & prepare for investigation -> pull out information or even build up visualization\n",
    "- c) **Model Training:** load data in memory (partially) -> train model(s) -> store model (for large model computations) or store results (for fast model trainings) (again: apply rules for storing data)\n",
    "- d) **Evaluation:** load computed results in memory -> extract and prepare data for visualization -> configure visualization parameters -> produce visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conda Environment Reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why using conda or a dependency manager?\n",
    "- easily manage your dependencies, e.g. a library you are using to compute models or to visualize data\n",
    "- share the managed environment with your team mates\n",
    "- force your team to make your work reproducible. If you are checking the repository out in a year, you can quickly install the whole experiment again, even if your computer setup changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment file we use for this Data Science Lab demonstration notebooks:\n",
    "```yaml\n",
    "name: sur-dsl2020\n",
    "channels:\n",
    "- defaults\n",
    "dependencies:\n",
    "- python>=3.8\n",
    "- jupyter\n",
    "- matplotlib\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit-learn\n",
    "- seaborn\n",
    "- tqdm\n",
    "- pip>=20.0\n",
    "- pip:\n",
    "  - kaggle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start off with creating an environment.yml for conda.\n",
    "\n",
    "*environment.yml*\n",
    "```yaml\n",
    "name: sur-dsl2020-corona-forecasting\n",
    "channels:\n",
    "- defaults\n",
    "dependencies:\n",
    "- python>=3.7\n",
    "- jupyter\n",
    "- matplotlib\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit-learn\n",
    "- seaborn\n",
    "```\n",
    "\n",
    "If you have just downloaded the team repository or just created the *environment.yml* freshly, use ``conda env create -f environment.yml`` in the local directory to create a python environment for the purpose of your Data Science Lab experiments.\n",
    "Activate the environment with ``conda activate sur-dsl2020-corona-forecasting`` (or the particular name you gave your shared environment).\n",
    "Make sure to not add your local path for the environment in the environment.yml -- it might be different for your team mates environment.\n",
    "\n",
    "\n",
    "If you want to add a new library to your experiment, add it first in *environment.yml*:\n",
    "\n",
    "*environment.yml*\n",
    "```yaml\n",
    "name: sur-dsl2020-corona-forecasting\n",
    "channels:\n",
    "- defaults\n",
    "dependencies:\n",
    "- python>=3.7\n",
    "- jupyter\n",
    "- matplotlib\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit-learn\n",
    "- seaborn\n",
    "- tqdm\n",
    "```\n",
    "\n",
    "After adding a new library, make sure you have activated your environment and then invoke an update in your experiment working directory by calling ``conda env update`` (which uses the local *./environment.yml* to update your python-conda-environment).\n",
    "\n",
    "You can also add pip packages (if they are not available in conda).\n",
    "Check first for availability in conda by calling ``conda search <package>``, e.g. ``conda search kaggle`` (kaggle is currently not available on conda).\n",
    "Then you can search for it in pip: ``pip search kaggle`` and if you now want to add it to the environment, you can specify it with:\n",
    "\n",
    "*environment.yml*\n",
    "```yaml\n",
    "name: sur-dsl2020-corona-forecasting\n",
    "channels:\n",
    "- defaults\n",
    "dependencies:\n",
    "- python>=3.7\n",
    "- jupyter\n",
    "- matplotlib\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit-learn\n",
    "- seaborn\n",
    "- tqdm\n",
    "- pip>=20.0\n",
    "- pip:\n",
    "  - kaggle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Prototyping to Polished Results\n",
    "Initially you might have no idea which library to use, what kind of data is useful and which choice of visualization technique might work best to present your results.\n",
    "Jupyter notebooks (such as this one) are very nice for fast prototyping, inspecting data, inspecting functional behaviour or creating pretty visualizations.\n",
    "But your code can get quite messy after some time.\n",
    "This is due to the circumstance that you are mostly exploring new terrain.\n",
    "To make your work easier accessible for you and your team mates it helps to get from prototyping to polished results from time to time.\n",
    "This is inspired from agile development and means that you are first quickly prototyping and after some phases return to separating those prototypes into polished results.\n",
    "A polished result could be e.g. a routine (a short python script and an explanation) to retrieve data or it could be one single kind of visualization of your data (e.g. a notebook which is solely meant for creating that nice boxplots that you intend to use in your report)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data retrieval routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example description for the kaggle titanic dataset:**\n",
    "\n",
    "Download the kaggle dataset:\n",
    "- Invoke below command\n",
    "- Make sure kaggle is installed in your current environment. Have a look on their [github page](https://github.com/Kaggle/kaggle-api)\n",
    "- Make sure your kaggle credentials are stored in your home folder under *.kaggle/kaggle.json* (it might give you \"*403 - Forbidden*\" otherwise) - you can download the file from your account page on https://www.kaggle.com/{username}/account\n",
    "- Make sure you accepted the rules for the particular dataset, e.g. on https://www.kaggle.com/c/titanic/rules (otherwise you might also get a \"*403 - Forbidden*\" response)\n",
    "\n",
    "Put your credentials into *~/.kaggle/kaggle.json* and apply correct access rights to them with ``chmod 600 ~/.kaggle/kaggle.json``\n",
    "\n",
    "Then download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/julian/.kaggle/kaggle.json'\n",
      "Downloading titanic.zip to /home/julian/projects/teaching/20ss-Data-Science-Lab/course-experiment-skeleton\n",
      "  0%|                                               | 0.00/34.1k [00:00<?, ?B/s]\n",
      "100%|███████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 556kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                  experiment-workflows.ipynb\r\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m                       \u001b[1m\u001b[36msrc\u001b[m\u001b[m\r\n",
      "environment.yml            sur-dsl2020-skeleton\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open titanic.zip, titanic.zip.zip or titanic.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "!unzip titanic.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype First Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'data/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/train.csv does not exist: 'data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f5943a8d9379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data into memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# df is short for dataframe, a common, but sadly generic name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sur-dsl2020-skeleton/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sur-dsl2020-skeleton/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sur-dsl2020-skeleton/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sur-dsl2020-skeleton/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sur-dsl2020-skeleton/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File data/train.csv does not exist: 'data/train.csv'"
     ]
    }
   ],
   "source": [
    "# Load data into memory\n",
    "df = pd.read_csv(data_file)  # df is short for dataframe, a common, but sadly generic name\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think about what aspect you want to visualize\n",
    "# As an example, we want to get an impression of the distribution of a certain feature\n",
    "sns.boxplot(df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try to iterate over our visualization to plot it visually appealing\n",
    "# and in a way so we can actually use it in out report\n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Age distribution in original Titanic dataset')\n",
    "sns.boxplot(df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to store the figure locally in a common dir\n",
    "visualization_path = 'vis/2020-05-titanic-age-distribution.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you actually have a subdirectory for visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir vis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the figure locally (but only put into git if expensive to compute - the routine is important)\n",
    "fig.savefig(visualization_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe you notice, that the figure might be too small in resolution for your report, so increase the resolution:\n",
    "fig.savefig(visualization_path, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you might notice, that the bottom axis label was cut off, so we might need to adjust our subplot:\n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Age distribution in original Titanic dataset')\n",
    "sns.boxplot(df['Age'])\n",
    "plt.gcf().subplots_adjust(bottom=0.2)  # We added this adjustment line\n",
    "fig.savefig(visualization_path, dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assemble all of this particular boxplot visualization into a separated visualization-generation routine given that we have locally the titanic data available.\n",
    "The below cell can be a standalone python script *visualize_titanic_age.py* and you can check if the cell is actually working by restarting the kernel of the jupyter notebook and invoking the cell independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "visualization_base_path = 'vis/'\n",
    "data_file = 'data/train.csv'\n",
    "\n",
    "# Prepare path to save to\n",
    "if not os.path.exists(visualization_base_path):\n",
    "    os.makedirs(visualization_base_path)\n",
    "if not os.path.isdir(visualization_base_path):\n",
    "    raise ValueError('<{p}> is not a valid path to store visualizations to'.format(p=visualization_base_path))\n",
    "\n",
    "date_prefix = datetime.datetime.today().strftime(\"%Y-%m-%d\")\n",
    "visualization_name = '{prefix}-titanic-age-distribution.png'.format(prefix=date_prefix)\n",
    "save_path = os.path.join(visualization_base_path, visualization_name)\n",
    "\n",
    "# Load the titanic train data into memory\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# Initialize seaborn and matplotlib config\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Create the boxplot\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(df['Age'])\n",
    "\n",
    "# Apply figure configs after we created the plots\n",
    "plt.gcf().subplots_adjust(bottom=0.2)\n",
    "ax.set_title('Age distribution in original Titanic dataset')\n",
    "\n",
    "# Store the visualization result with high resolution\n",
    "fig.savefig(save_path, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
